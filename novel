# -*- coding: utf-8 -*-
#python 2.7



import sys
import time
import re
import random
import requests
from bs4 import BeautifulSoup
class crawler():

    def __init__(self):
        self.headers = {
    'Connection': "keep-alive",
    'Pragma': "no-cache",
    'Cache-Control': "no-cache",
    'Origin': "https://www.po18.tw",
    'Upgrade-Insecure-Requests': "1",
    'Content-Type': "application/x-www-form-urlencoded",
    'User-Agent': "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36",
    'Accept': "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
    'Referer': "https://www.po18.tw/search/index",
    'Accept-Encoding': "gzip, deflate, br",
    'Accept-Language': "zh-CN,zh;q=0.9",
            'Cookie': "_ga=GA1.2.558445314.1548900759; authtoken6=1; bgcolor=bg-default; word=select-m; authtoken1=enp6eXl5MTIzNDU2; _paabbcc=f9u2576mp2u5e9raus2marogm5; _po18rf-tk001=58a7902b0e414d9a8546a82552b0d3efe662a2b689a1eda770ce6a61df74bcdea%3A2%3A%7Bi%3A0%3Bs%3A13%3A%22_po18rf-tk001%22%3Bi%3A1%3Bs%3A32%3A%22qL-9i3uoSCF7P792vl5_CmNWRrT-uttj%22%3B%7D; _gid=GA1.2.1708253480.1552892067; po18Limit=6d4c449fc3268973f92df90aad9af59af0885074fe02e29be9cf3d42d091ec89a%3A2%3A%7Bi%3A0%3Bs%3A9%3A%22po18Limit%22%3Bi%3A1%3Bs%3A1%3A%221%22%3B%7D; url=https%3A%2F%2Fwww.po18.tw; authtoken2=ZGE4ZGY2NTI5YmE5MDQ3MzBhZmM3MzVlNzU3NTNiMjc%3D; authtoken3=1712433363; authtoken4=1196903873; authtoken5=1552892381",
    'cache-control': "no-cache",
    'Postman-Token': "39cd69c7-a5a7-49e8-9e3a-fc74cd6d965e"
    }

        self.session = requests.Session()
        self.wait_time=7

    def login(self):
        try:
            account = raw_input(u"请输入账号：").decode(sys.stdin.encoding)
            pwd = raw_input(u"请输入密码：").decode(sys.stdin.encoding)
            headers = {
                'Connection': "keep-alive",
                'Pragma': "no-cache",
                'Cache-Control': "no-cache",
                'Origin': "https://members.po18.tw",
                'Upgrade-Insecure-Requests': "1",
                'Content-Type': "application/x-www-form-urlencoded",
                'User-Agent': "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36",
                'Accept': "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
                'Referer': "https://members.po18.tw/apps/login.php",
                'Accept-Encoding': "gzip, deflate, br",
                'Accept-Language': "zh-CN,zh;q=0.9",
                'cache-control': "no-cache",
                'Postman-Token': "9e794009-96a6-4d22-be14-4b128cd7bd41"
            }

            data = {
                'account': account,
                'pwd': pwd,
                'remember_me': '1',
                'comefrom_id': '2',
                'owner': 'COC',
                'front_events_id': '',
                'front_events_name': '',
                'client_ip': '10.55.66.233',
                'url': 'https://www.po18.tw'
            }
            login_url = 'https://members.po18.tw/apps/login.php'
            login_html = self.session.post(login_url, data=data,timeout=self.wait_time,headers=headers)
            if login_html.url == 'https://members.po18.tw/apps/login.php':
                print '账号或密码错误，请重新输入'
                self.login()
            elif login_html.url == 'https://www.po18.tw':
                print  '登陆成功'
            else:
                print '---error---'
        except requests.exceptions.ConnectionError:
            print 'ConnectionError，请重新登陆'
            self.login()
        except requests.exceptions.ReadTimeout or RuntimeError:
            print '登陆超时，重新登陆'
            self.login()


    def get_author_book(self,author_name,author_url):
        #print author_url
        try:
            headers = {
                'Host': 'www.po18.tw',
                'Upgrade-Insecure-Requests': '1',
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
            }
            result = []
            html = self.session.get(author_url, headers=headers,timeout=self.wait_time).text
            soup = BeautifulSoup(html, 'lxml')
            for i in soup.find('div', class_='work_list').find_all('div', class_='book_info'):
                one_result = []
                book_name1 = i.find('div', class_='book_name').find('a').text
                des1 = i.find('dd', class_='intro').text
                book_url1 = i.find('div', class_='book_name').find('a').get(
                    'href') + '/articles'
                print book_url1
                one_result.append(book_name1)
                one_result.append(des1)
                one_result.append(book_url1)
                result.append(one_result)
            return result
        except requests.exceptions.ConnectionError:
            self.get_author_book(author_name,author_url)
            print '重新获得%s的系列书籍'%author_name.encode('utf-8')
        except requests.exceptions.ReadTimeout or RuntimeError:
            self.get_author_book(author_name, author_url)
            print '搜索超时，重新搜索中......'




    def search(self):
        try:
            k=0
            url = 'https://www.po18.tw/search/index'
            html = self.session.get(url).text
            soup = BeautifulSoup(html, 'lxml')
            # po18rf_token = soup.find("input", attrs={'name': "_po18rf-token"}).get('value')
            search_name = raw_input(u"请输入关键字：").decode(sys.stdin.encoding)
            search_type = {
                '全部': 'all',
                '作家': 'author',
                '书籍': 'book'
            }
            book_dict = {}
            total_page=2
            search_page=1
            while search_page<total_page:
                #print search_page
                headers = {
                    'Connection': "keep-alive",
                    'Pragma': "no-cache",
                    'Cache-Control': "no-cache",
                    'Origin': "https://www.po18.tw",
                    'Upgrade-Insecure-Requests': "1",
                    'Content-Type': "application/x-www-form-urlencoded",
                    'User-Agent': "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36",
                    'Accept': "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
                    'Referer': "https://www.po18.tw/search/index",
                    'Accept-Encoding': "gzip, deflate, br",
                    'Accept-Language': "zh-CN,zh;q=0.9",
                    'Cookie': "_ga=GA1.2.558445314.1548900759; authtoken6=1; bgcolor=bg-default; word=select-m; authtoken1=enp6eXl5MTIzNDU2; _paabbcc=f9u2576mp2u5e9raus2marogm5; _po18rf-tk001=58a7902b0e414d9a8546a82552b0d3efe662a2b689a1eda770ce6a61df74bcdea%3A2%3A%7Bi%3A0%3Bs%3A13%3A%22_po18rf-tk001%22%3Bi%3A1%3Bs%3A32%3A%22qL-9i3uoSCF7P792vl5_CmNWRrT-uttj%22%3B%7D; _gid=GA1.2.1708253480.1552892067; po18Limit=6d4c449fc3268973f92df90aad9af59af0885074fe02e29be9cf3d42d091ec89a%3A2%3A%7Bi%3A0%3Bs%3A9%3A%22po18Limit%22%3Bi%3A1%3Bs%3A1%3A%221%22%3B%7D; url=https%3A%2F%2Fwww.po18.tw; authtoken2=ZGE4ZGY2NTI5YmE5MDQ3MzBhZmM3MzVlNzU3NTNiMjc%3D; authtoken3=1712433363; authtoken4=1196903873; authtoken5=1552892381",
                    'cache-control': "no-cache",
                    'Postman-Token': "39cd69c7-a5a7-49e8-9e3a-fc74cd6d965e"
                }
                data = {
                    '_po18rf-tk001': '_B3ubdEpdkVZeUlR2-l2rOa5FnjITrQBy1yzk9NYQoKNUcNUuBoDKgo6D2aL3k-ekNUjJ4sj-laZLue-piw26A==',
                    'name': search_name,
                    'searchtype': 'all',
                    'page': str(search_page).decode('utf-8') + '&page1=undefined',
                    'page1': 'undefined'
                }
                html = self.session.post(url, data=data, headers=headers, timeout=self.wait_time).text
                # print html
                soup = BeautifulSoup(html, 'lxml')
                try:
                    v = int(soup.find('div', class_='result_list', attrs={'id': "BOOK"}).find('h2').find('span').text)
                    if v % 10 == 0:
                        total_page = v / 10
                    else:
                        total_page = v/10 + 2
                    if total_page>3:
                        total_page=3
                except:
                    print '~~~~~~~~~~~~~~~~~'
                    print '无法找到相关内容！'
                    print '请重新搜索！'
                    print '~~~~~~~~~~~~~~~~~'
                    self.search()
                #total_page=int(raw_input(u"一共搜索到%s本书籍，请输入搜索页数：   "%v).decode(sys.stdin.encoding))+1
                try:
                    author = soup.find('div', attrs={'id': "AUTHOR"}).find('h2').text
                    author_inf = soup.find_all('div', class_='author_info')
                    for i in range(len(author_inf)):
                        name = author_inf[i].find('div', class_='author_name').find('a').text
                        des = author_inf[i].find('dd', class_='about_author').text
                        author_url = 'https://www.po18.tw' + author_inf[i].find('div', class_='author_name').find(
                            'a').get(
                            'href')
                        result = self.get_author_book(name, author_url)
                        print u"作者姓名： " + name
                        print u"作者简介： " + des
                        try:
                            for j in result:
                                print u'该作者的系列小说：   ' + j[0] + '      ' + u'简介：   ' + j[1]
                                book_dict[j[0]] = j[2]
                        except TypeError:
                            pass
                except AttributeError:
                    k = k + 1
                    #print 'k:   ', k
                    print soup.find('div', attrs={'id': "AUTHOR"}).text
                try:
                    book = soup.find('div', attrs={'id': "BOOK"}).find('h2').text
                    book_inf = soup.find_all('div', class_='book_info')
                    for i in range(len(book_inf)):
                        book_name = book_inf[i].find('div', class_='book_name').find('a').text
                        book_author = book_inf[i].find('div', class_='book_author').find('a').text
                        des = book_inf[i].find('div', class_='intro').text
                        book_dict[book_name] = 'https://www.po18.tw' + book_inf[i].find('div', class_='book_name').find(
                            'a').get(
                            'href') + '/articles'
                        print u"书名： " + book_name
                        print u"作者： " + book_author
                        print u"简介： " + des
                except AttributeError:
                    k = k + 1
                    #print 'k:   ', k
                    print soup.find('div', attrs={'id': 'BOOK'}).text
                if k == 2*search_page:
                    #print 'k:   ',k
                    print '无法找到相关内容！'
                    print '请重新搜索！'
                    book_dict=self.search()
                    break
                search_page=search_page+1
                print book_dict
            return book_dict
        except requests.exceptions.ConnectionError:
            print '链接错误，请重新搜索'
            self.search()
        except requests.exceptions.ReadTimeout or RuntimeError:
            self.search()
            print '搜索超时，重新搜索中......'




    def insert_ad(self):
        ad_path=u'/Users/zuoyiya/Desktop/2.txt'
        ad_list=[]
        t=[]
        with open(ad_path,'r') as a:
            for i in a.readlines():
                if 'ad' in i:
                    ad_list.append(t)
                    t = []
                else:
                    if i == '\n':
                        t.append('\n')
                    else:
                        t.append(i.replace('\n', '').decode('utf-8'))
        a.close()
        return ad_list[1:]




    def chapter_content(self,chapterurl,title,type,nu):
        try:
            # url = "https://www.po18.tw/books/666652/articlescontent/7631907"
            url = chapterurl.replace("articles", "articlescontent")
            if u'免費閱讀'==type:
                coo=["authToken=2ulk99qiju6q1auq1t9u0jrs14; _ga=GA1.2.327249816.1548928435; _gid=GA1.2.368859719.1548928435; url=https%3A%2F%2Fwww.po18.tw; authtoken1=enp6eXl5MTIzNDU2; authtoken2=MjdmY2ZiMzE0ZjM3NzYzMzY2Yzk2Y2E2OGEwNDQzMTk%3D; authtoken3=1712433363; authtoken4=3697529318; authtoken5=1548928452; authtoken6=1; _po18rf-token=f19fb4bd5947e47a8ad4bc29a70ceb8720a7e133bfcb51b47870adb7f92c5aa8a%3A2%3A%7Bi%3A0%3Bs%3A13%3A%22_po18rf-token%22%3Bi%3A1%3Bs%3A32%3A%22-NbZMI2Mozyxj_X012cQtSSc-sRRNDhz%22%3B%7D; _gat_gtag_UA_11633339_26=1; bgcolor=bg-default; word=select-m",
                     'authToken=nb5tvum3gg5aq7cl4umdtgr524; _po18rf-token=20985fb4cc41d9fdeeb2781aae404bf30d882c4681a7fa2df32cc70ee00d3788a%3A2%3A%7Bi%3A0%3Bs%3A13%3A%22_po18rf-token%22%3Bi%3A1%3Bs%3A32%3A%22JTET5c8V4-1vkmSYMD9Cdor2d9iG3OxP%22%3B%7D; _ga=GA1.2.558445314.1548900759; _gid=GA1.2.1735490823.1548900759; authtoken1=enp6eXl5MTIzNDU2; authtoken6=1; bgcolor=bg-default; word=select-m; url=https%3A%2F%2Fwww.po18.tw%2Fbooks%2F661373%2Farticles%2F7574637; authtoken2=Y2VlOGI4YjFlY2E4ZjAzNGMxMDQ3MTgwMjIxZTNhNzA%3D; authtoken3=1712433363; authtoken4=233406403; authtoken5=1548989298; _gat_gtag_UA_11633339_26=1',
                     "authToken=nb5tvum3gg5aq7cl4umdtgr524; _po18rf-token=20985fb4cc41d9fdeeb2781aae404bf30d882c4681a7fa2df32cc70ee00d3788a%3A2%3A%7Bi%3A0%3Bs%3A13%3A%22_po18rf-token%22%3Bi%3A1%3Bs%3A32%3A%22JTET5c8V4-1vkmSYMD9Cdor2d9iG3OxP%22%3B%7D; _ga=GA1.2.558445314.1548900759; _gid=GA1.2.1735490823.1548900759; authtoken6=1; bgcolor=bg-default; word=select-m; po18Limit=6d4c449fc3268973f92df90aad9af59af0885074fe02e29be9cf3d42d091ec89a%3A2%3A%7Bi%3A0%3Bs%3A9%3A%22po18Limit%22%3Bi%3A1%3Bs%3A1%3A%221%22%3B%7D; url=https%3A%2F%2Fwww.po18.tw%2F; authtoken1=bHVvbHVvMTM3MTQy; authtoken2=ZGI5NjQ5ZDdmMzZmODliM2Q2MTM0Y2FhZDBkOTA1Y2M%3D; authtoken3=3355602036; authtoken4=889142853; authtoken5=1549008835; _gat_gtag_UA_11633339_26=1"]
                headers = {
                    'Referer': chapterurl,
                    'Cookie': random.choice(coo)
                    # 'cache-control': "no-cache",
                    # 'Postman-Token': "d637de9a-b2be-48b2-a727-34b8b76abc19"
                }
            elif u'閱讀'==type:
                cookie=['authToken=nb5tvum3gg5aq7cl4umdtgr524; _po18rf-token=20985fb4cc41d9fdeeb2781aae404bf30d882c4681a7fa2df32cc70ee00d3788a%3A2%3A%7Bi%3A0%3Bs%3A13%3A%22_po18rf-token%22%3Bi%3A1%3Bs%3A32%3A%22JTET5c8V4-1vkmSYMD9Cdor2d9iG3OxP%22%3B%7D; _ga=GA1.2.558445314.1548900759; _gid=GA1.2.1735490823.1548900759; authtoken6=1; bgcolor=bg-default; word=select-m; po18Limit=6d4c449fc3268973f92df90aad9af59af0885074fe02e29be9cf3d42d091ec89a%3A2%3A%7Bi%3A0%3Bs%3A9%3A%22po18Limit%22%3Bi%3A1%3Bs%3A1%3A%221%22%3B%7D; url=https%3A%2F%2Fwww.po18.tw%2F; authtoken1=bHVvbHVvMTM3MTQy; authtoken2=ZGI5NjQ5ZDdmMzZmODliM2Q2MTM0Y2FhZDBkOTA1Y2M%3D; authtoken3=3355602036; authtoken4=889142853; authtoken5=1549008835; _gat_gtag_UA_11633339_26=1',
                        'authToken=nb5tvum3gg5aq7cl4umdtgr524; _po18rf-token=20985fb4cc41d9fdeeb2781aae404bf30d882c4681a7fa2df32cc70ee00d3788a%3A2%3A%7Bi%3A0%3Bs%3A13%3A%22_po18rf-token%22%3Bi%3A1%3Bs%3A32%3A%22JTET5c8V4-1vkmSYMD9Cdor2d9iG3OxP%22%3B%7D; _ga=GA1.2.558445314.1548900759; _gid=GA1.2.1735490823.1548900759; authtoken6=1; bgcolor=bg-default; word=select-m; po18Limit=6d4c449fc3268973f92df90aad9af59af0885074fe02e29be9cf3d42d091ec89a%3A2%3A%7Bi%3A0%3Bs%3A9%3A%22po18Limit%22%3Bi%3A1%3Bs%3A1%3A%221%22%3B%7D; url=https%3A%2F%2Fwww.po18.tw%2F; authtoken1=bHVvbHVvMTM3MTQy; authtoken2=ZGI5NjQ5ZDdmMzZmODliM2Q2MTM0Y2FhZDBkOTA1Y2M%3D; authtoken3=3355602036; authtoken4=889142853; authtoken5=1549008835; _gat_gtag_UA_11633339_26=',
                        'authToken=nb5tvum3gg5aq7cl4umdtgr524; _po18rf-token=20985fb4cc41d9fdeeb2781aae404bf30d882c4681a7fa2df32cc70ee00d3788a%3A2%3A%7Bi%3A0%3Bs%3A13%3A%22_po18rf-token%22%3Bi%3A1%3Bs%3A32%3A%22JTET5c8V4-1vkmSYMD9Cdor2d9iG3OxP%22%3B%7D; _ga=GA1.2.558445314.1548900759; _gid=GA1.2.1735490823.1548900759; authtoken6=1; bgcolor=bg-default; word=select-m; po18Limit=6d4c449fc3268973f92df90aad9af59af0885074fe02e29be9cf3d42d091ec89a%3A2%3A%7Bi%3A0%3Bs%3A9%3A%22po18Limit%22%3Bi%3A1%3Bs%3A1%3A%221%22%3B%7D; url=https%3A%2F%2Fwww.po18.tw%2F; authtoken1=bHVvbHVvMTM3MTQy; authtoken2=ZGI5NjQ5ZDdmMzZmODliM2Q2MTM0Y2FhZDBkOTA1Y2M%3D; authtoken3=3355602036; authtoken4=889142853; authtoken5=1549008835; _gat_gtag_UA_11633339_26=1']
                headers = {
                    'Referer': chapterurl,
                    'Cookie':random.choice(cookie)
                    # 'Cookie': 'authToken=nb5tvum3gg5aq7cl4umdtgr524; _po18rf-token=20985fb4cc41d9fdeeb2781aae404bf30d882c4681a7fa2df32cc70ee00d3788a%3A2%3A%7Bi%3A0%3Bs%3A13%3A%22_po18rf-token%22%3Bi%3A1%3Bs%3A32%3A%22JTET5c8V4-1vkmSYMD9Cdor2d9iG3OxP%22%3B%7D; _ga=GA1.2.558445314.1548900759; _gid=GA1.2.1735490823.1548900759; authtoken1=enp6eXl5MTIzNDU2; authtoken6=1; bgcolor=bg-default; word=select-m; url=https%3A%2F%2Fwww.po18.tw%2Fbooks%2F661373%2Farticles%2F7574637; authtoken2=Y2VlOGI4YjFlY2E4ZjAzNGMxMDQ3MTgwMjIxZTNhNzA%3D; authtoken3=1712433363; authtoken4=233406403; authtoken5=1548989298; _gat_gtag_UA_11633339_26=1',
                    # 'cache-control': "no-cache",
                    # 'Postman-Token': "d637de9a-b2be-48b2-a727-34b8b76abc19"
                }
            response = self.session.get(url, headers=headers,timeout=self.wait_time)
            chp_con = response.text.replace("&nbsp;", '')
            soup = BeautifulSoup(chp_con, 'lxml')
            if len(soup.find_all('p'))<10:
                #print chp_con
                nu = nu - 1
                print '---error---,%s开始第%d次下载'%(title.encode('utf-8'),11-nu)
                chp_con=self.chapter_content(chapterurl, title, type,nu)
            if nu:
                pass
            return chp_con
        except requests.exceptions.ConnectionError :
            print '重新下载%s' % title.encode('utf-8')
            self.chapter_content(chapterurl,title,type,nu)
        except requests.exceptions.ReadTimeout or RuntimeError:
            print '请求超时，请重新下载%s' % title.encode('utf-8')
            # time.sleep(6)
            self.chapter_content(chapterurl,title,type,nu)



    def get_chapter(self,bookname,chapterurl,title,filepath,type,nu,ad):
        try:
            t = self.chapter_content(chapterurl, title, type, nu)
            soup = BeautifulSoup(t, 'lxml')
            bookname = bookname.replace('/', '')
            with open(filepath + u"{}.txt".format(bookname), "a") as f:
                f.write(title.encode('utf-8'))
                for i in soup.find_all('p'):
                    f.write(i.text.encode('utf-8').strip() + '\n')
                for j in random.choice(ad):
                    f.write(j.encode('utf-8') +'\n')
            print '%s--->下载完毕' % (title.encode('utf-8'))
            f.close()
            # time.sleep(2)
        except TypeError:
            self.get_chapter(bookname,chapterurl,title,filepath,type,nu,ad)



    def get_book_url(self,book_url):
        try:
            l=[]
            book_name = raw_input(u"请输入你想下载的书本名：").decode(sys.stdin.encoding)
            u=book_url[book_name]
            l.append(book_name)
            l.append(u)
            return l
        except Exception:
            print '输入错误，请重新输入'
            self.get_book_url(book_url)


    def pages_cha(self):
        cha_num = raw_input('请输入章节序号：').decode(sys.stdin.encoding)
        if cha_num == u'all':
            pages = 1
            cha_num = 0
        elif int(cha_num) / 100 == 0 and int(cha_num) >= 100:
            pages = int(cha_num) / 100
            cha_num = int(cha_num)
        else:
            pages = int(cha_num) / 100 + 1
            cha_num = int(cha_num)
        return pages,cha_num



    def get_book(self):
        book_urls = self.search()
        li=self.get_book_url(book_urls)
        book_name=li[0]
        book_url=li[1]
        ad=self.insert_ad()
        #filepath = raw_input('请输入存储路径：').decode(sys.stdin.encoding)
        filepath='\Users\zuoyiya\Downloads'
        try:
            pages, cha_num = self.pages_cha()
            total=100
            while total-pages:
                url=book_url+'?page={}'.format(str(pages))
                headers = {
                    'Host': 'www.po18.tw',
                    'Upgrade-Insecure-Requests': '1',
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
                }
                # url='https://www.po18.tw/books/597700/articles'
                # print url
                html = self.session.get(url, headers=headers,timeout=self.wait_time).text
                soup = BeautifulSoup(html, 'lxml')
                try:
                    v=int(re.findall(u'目前(.*?)章回',soup.find('dd',class_='statu').find('span').text)[0])
                    total=v/100+2
                    if cha_num>v:
                        print '一共%d章'%v
                        pages, cha_num = self.pages_cha()
                    print '一共%d页，开始抓取%d页'%(total-1,pages)
                except Exception:
                    pass
                for i in soup.find('div', class_='list-view').find_all('div', class_='c_l'):
                    if i.find('div', class_='l_chaptname').find('img'):
                        continue
                    chapter_name = i.find('div', class_='l_chaptname').text
                    chapter_counter = i.find('div', class_='l_counter').text
                    title = chapter_counter + chapter_name
                    if int(chapter_counter)>=cha_num:
                        try:
                            global nu
                            nu = 10
                            chapter_url = 'https://www.po18.tw' + i.find('div', class_='l_btn').find('a').get('href')
                            type = i.find('a', class_='btn_L_blue').text
                            h = self.get_chapter(book_name, chapter_url, title, filepath, type, nu, ad)
                        except AttributeError:
                            print '%s为付费章节' % (title.encode('utf-8'))
                    else:
                        continue
                time.sleep(1)
                pages=pages+1
        except requests.exceptions.ConnectionError:
            self.get_book()
            print '重新下载--->%s'%book_name.encode('utf-8')
        except requests.exceptions.ReadTimeout or RuntimeError:
            print '请求超时，重新下载--->%s'%book_name.encode('utf-8')
            time.sleep(3)
            self.get_book()



if __name__=="__main__":
    cra=crawler()
    log=cra.login()
    action=1
    while int(action):
        cra.get_book()
        time.sleep(3)
        action = raw_input('继续搜索：1/停止搜索：0').decode(sys.stdin.encoding)



'''

1.用户名和密码分别在代码的37行和38行


2.请求的响应时间在代码的33行；觉得慢了可以减小，不建议太小！


3.下载好的小说存储路径在353行


4.插入广告时，广告的读取路径在234行


5.下载小说时，下载整本输入all，从第多少章下载的话就输入该章的序列好，比如《迟早（限）》这本书，输入19，就从0019真淫荡啊开始下载


6.等下载好一本后，想继续下载输入1，想结束代码输入0


7.广告格式：ad为广告分割点
ad
对你的
～～～
ad
Due
~~~
ad
ekfbdonkey带劲儿
～～～
ad
'''


